{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import os\n",
    "import wandb\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from peft import get_peft_model, LoraConfig\n",
    "wandb.login(key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7310b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"root_dir\": '',\n",
    "    \"image_size\": 224,\n",
    "    \"embedding_dim\": 768, \n",
    "    \"num_workers\": 2, \n",
    "    \"vit_dropout\": 0.1,\n",
    "    # LoRA Config\n",
    "    \"lora_r\": 64,\n",
    "    \"lora_alpha\": 256,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    # Phase 1: Contrastive Pre-training\n",
    "    \"batch_size_contrastive\": 32,\n",
    "    \"projection_dim\": 128,\n",
    "    \"lr_contrastive\": 2e-4,\n",
    "    \"wd_contrastive\": 1e-6,\n",
    "    \"epochs_contrastive\": 10,\n",
    "    \"temperature\": 0.07,\n",
    "    # Phase 2: Supervised Fine-tuning\n",
    "    \"batch_size_finetune\": 32,\n",
    "    \"lr_finetune\": 1e-4,\n",
    "    \"wd_finetune\": 1e-5,\n",
    "    \"epochs_finetune\": 15,\n",
    "    \"early_stopping_patience\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f52fd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_BACKBONE_PATH = \"./vit-contrastive-lora-backbone.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f36fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data and clean image path\n",
    "test = pd.read_csv('u0_test.csv', index_col=0)\n",
    "train_df = pd.read_csv('u0_train.csv', index_col=0)\n",
    "val_df = pd.read_csv('u0_val.csv', index_col=0)\n",
    "\n",
    "train_df.index = train_df.index.str.replace('CheXpert-v1.0-small', 'chexpert')\n",
    "test.index = test.index.str.replace('CheXpert-v1.0-small', 'chexpert')\n",
    "val_df.index = val_df.index.str.replace('CheXpert-v1.0-small', 'chexpert')\n",
    "\n",
    "class_names = train_df.columns.tolist()\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Validation data shape: {val_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24808141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    \"\"\"Returns a dictionary of augmentation pipelines for each phase.\"\"\"\n",
    "    normalize_transform = A.Normalize(\n",
    "        mean=[0.506, 0.506, 0.506],\n",
    "        std=[0.287, 0.287, 0.287]\n",
    "    )\n",
    "\n",
    "    base_train_transforms = [\n",
    "        A.Affine(scale=(0.95, 1.05), p=0.5),\n",
    "        A.OneOf([A.Affine(rotate=(-20, 20), p=0.5), A.Affine(shear=(-5, 5), p=0.5)], p=0.5),\n",
    "        A.Affine(translate_percent=(-0.05, 0.05), p=0.5),\n",
    "        A.Resize(224, 224),\n",
    "        normalize_transform,\n",
    "        ToTensorV2()\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        'contrastive': A.Compose(base_train_transforms),\n",
    "        'supervised': A.Compose(base_train_transforms),\n",
    "        'validation': A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            normalize_transform,\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    }\n",
    "def get_weighted_sampler(data_frame):\n",
    "    \"\"\"\n",
    "    Creates a WeightedRandomSampler to handle class imbalance.\n",
    "    It gives more weight to samples from under-represented classes.\n",
    "    \"\"\"\n",
    "    class_weights = (1.0 / data_frame.sum()).values\n",
    "    sample_weights = data_frame.dot(class_weights)\n",
    "    return WeightedRandomSampler(\n",
    "        weights=torch.tensor(sample_weights.values, dtype=torch.float),\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f5472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classes for contrastive learning, contrastive augmentations and contrastive loss\n",
    "\n",
    "class ContrastiveViewDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates two augmented views of each image, in this case a pair of augmentations for contrastive learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_frame, root_dir, transform):\n",
    "        self.img_paths = [os.path.join(root_dir, path) for path in data_frame.index]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = np.array(Image.open(self.img_paths[idx]).convert(\"RGB\"))\n",
    "            view1 = self.transform(image=image)['image']\n",
    "            view2 = self.transform(image=image)['image']\n",
    "            return view1, view2\n",
    "        except (IOError, FileNotFoundError):\n",
    "            print(f\"Warning: Could not load image at {self.img_paths[idx]}. Returning zeros.\")\n",
    "            return torch.zeros((3, config[\"image_size\"], config[\"image_size\"])), torch.zeros((3, config[\"image_size\"], config[\"image_size\"]))\n",
    "\n",
    "\n",
    "class NTXentLoss(nn.Module):\n",
    "    \"\"\"Normalized Temperature-scaled Cross Entropy Loss.\"\"\"\n",
    "    def __init__(self, temperature, device):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.shape[0]\n",
    "        z_i, z_j = F.normalize(z_i, p=2, dim=1), F.normalize(z_j, p=2, dim=1)\n",
    "        representations = torch.cat([z_i, z_j], dim=0)\n",
    "        similarity_matrix = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)\n",
    "        \n",
    "        labels = torch.cat([torch.arange(batch_size) for _ in range(2)]).to(self.device)\n",
    "        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "        \n",
    "        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(self.device)\n",
    "        labels, similarity_matrix = labels[~mask].view(labels.shape[0], -1), similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)\n",
    "        \n",
    "        positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)\n",
    "        negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)\n",
    "        \n",
    "        logits = torch.cat([positives, negatives], dim=1) / self.temperature\n",
    "        ground_truth = torch.zeros(logits.shape[0], dtype=torch.long).to(self.device)\n",
    "        return self.criterion(logits, ground_truth) / (2 * batch_size)\n",
    "\n",
    "class ContrastiveViT(nn.Module):\n",
    "    \"\"\"ViT with a projection head for contrastive learning.\"\"\"\n",
    "    def __init__(self, model_name=\"vit_base_patch16_224\", embedding_dim=768, projection_dim=128, drop_rate=0.1):\n",
    "        super(ContrastiveViT, self).__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True, drop_rate=drop_rate)\n",
    "        self.backbone.head = nn.Identity()\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim), nn.ReLU(inplace=True), nn.Linear(embedding_dim, projection_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.projection_head(self.backbone(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b486f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supervised learning phase classes\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Standard dataset for supervised learning.\n",
    "    Each call to __getitem__ returns an augmented image and its corresponding label.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_frame, root_dir, transform):\n",
    "        self.img_paths = [os.path.join(root_dir, path) for path in data_frame.index]\n",
    "        self.labels = torch.tensor(data_frame.values, dtype=torch.float32)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = np.array(Image.open(self.img_paths[idx]).convert(\"RGB\"))\n",
    "            image_tensor = self.transform(image=image)['image']\n",
    "            return image_tensor, self.labels[idx]\n",
    "        except (IOError, FileNotFoundError):\n",
    "            print(f\"Warning: Could not load image at {self.img_paths[idx]}. Returning zeros.\")\n",
    "            return torch.zeros((3, config[\"image_size\"], config[\"image_size\"])), torch.zeros(14)\n",
    "\n",
    "\n",
    "class FineTuningViT(nn.Module):\n",
    "    \"\"\"ViT with a classification head for fine-tuning.\"\"\"\n",
    "    def __init__(self, lora_config, num_classes=14, drop_rate=0.1):\n",
    "        super(FineTuningViT, self).__init__()\n",
    "        backbone = timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=0, drop_rate=drop_rate)\n",
    "        self.backbone = get_peft_model(backbone, lora_config)\n",
    "        self.classifier = nn.Linear(self.backbone.embed_dim, num_classes)\n",
    "\n",
    "    def load_from_pretrained(self, pretrained_path):\n",
    "        self.backbone.load_state_dict(torch.load(pretrained_path, map_location=DEVICE), strict=False)\n",
    "        print(f\"Loaded pre-trained backbone from {pretrained_path}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classifier(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002b6a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main training loop for both contrastive learning and supervised learning\n",
    "def contrastive_train(cfg, save_path):\n",
    "    \"\"\"PHASE 1: Self-supervised pre-training.\"\"\"\n",
    "    print(f\"Starting Phase 1: Contrastive Pre-training on {DEVICE}\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    all_transforms = get_transforms()\n",
    "    train_dataset = ContrastiveViewDataset(train_df, cfg[\"root_dir\"], all_transforms['contrastive'])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg[\"batch_size_contrastive\"], shuffle=True, num_workers=cfg[\"num_workers\"], pin_memory=True, drop_last=True)\n",
    "    \n",
    "    model = ContrastiveViT(\n",
    "        embedding_dim=cfg[\"embedding_dim\"], \n",
    "        projection_dim=cfg[\"projection_dim\"],\n",
    "        drop_rate=cfg[\"vit_dropout\"]\n",
    "    ).to(DEVICE)\n",
    "    lora_config = LoraConfig(r=cfg[\"lora_r\"], lora_alpha=cfg[\"lora_alpha\"], target_modules=[\"qkv\", \"proj\"], lora_dropout=cfg[\"lora_dropout\"], bias=\"none\")\n",
    "    model.backbone = get_peft_model(model.backbone, lora_config)\n",
    "    model.backbone.print_trainable_parameters()\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=cfg[\"lr_contrastive\"], weight_decay=cfg[\"wd_contrastive\"])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg[\"epochs_contrastive\"])\n",
    "    loss_fn = NTXentLoss(temperature=cfg[\"temperature\"], device=DEVICE)\n",
    "    scaler = torch.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
    "\n",
    "    wandb.init(project=\"ViT-CheXpert-Pipeline\", name=\"Phase1-Contrastive\", config=cfg)\n",
    "    for epoch in range(cfg[\"epochs_contrastive\"]):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg['epochs_contrastive']}\", unit=\"batch\")\n",
    "\n",
    "        for view1, view2 in pbar:\n",
    "            view1, view2 = view1.to(DEVICE), view2.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.autocast(device_type=DEVICE.type, enabled=(DEVICE.type == 'cuda')):\n",
    "                proj1, proj2 = model(view1), model(view2)\n",
    "                loss = loss_fn(proj1, proj2)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({\"Loss\": loss.item()})\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Avg Contrastive Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        wandb.log({\"epoch\": epoch, \"contrastive_loss\": avg_loss, \"lr_contrastive\": scheduler.get_last_lr()[0]})\n",
    "    \n",
    "    torch.save(model.backbone.state_dict(), save_path)\n",
    "    print(f\"Phase 1 finished. Pre-trained backbone saved to {save_path}\")\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "def supervised_finetune(cfg, pretrained_path):\n",
    "    \"\"\"PHASE 2: Supervised fine-tuning for classification.\"\"\"\n",
    "    print(f\"Starting Phase 2: Supervised Fine-tuning on {DEVICE}\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    all_transforms = get_transforms()\n",
    "    train_dataset = SupervisedDataset(train_df, cfg[\"root_dir\"], all_transforms['supervised'])\n",
    "    val_dataset = SupervisedDataset(val_df, cfg[\"root_dir\"], all_transforms['validation'])\n",
    "    \n",
    "    sampler = get_weighted_sampler(train_df)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg[\"batch_size_finetune\"], sampler=sampler, num_workers=cfg[\"num_workers\"], pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg[\"batch_size_finetune\"], shuffle=False, num_workers=cfg[\"num_workers\"], pin_memory=True)\n",
    "\n",
    "    lora_config = LoraConfig(r=cfg[\"lora_r\"], lora_alpha=cfg[\"lora_alpha\"], target_modules=[\"qkv\", \"proj\"], lora_dropout=cfg[\"lora_dropout\"], bias=\"none\")\n",
    "    model = FineTuningViT(lora_config, num_classes=14, drop_rate=cfg[\"vit_dropout\"])\n",
    "    model.load_from_pretrained(pretrained_path)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=cfg[\"lr_finetune\"], weight_decay=cfg[\"wd_finetune\"])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg[\"epochs_finetune\"])\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    metric = torchmetrics.AUROC(task=\"multilabel\", num_labels=14, average=None).to(DEVICE)\n",
    "    scaler = torch.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
    "\n",
    "    wandb.init(project=\"ViT-CheXpert-Pipeline\", name=\"Phase2-Finetune\", config=cfg)\n",
    "    best_val_auc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(cfg[\"epochs_finetune\"]):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg['epochs_finetune']}\", unit=\"batch\")\n",
    "        \n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.autocast(device_type=DEVICE.type, enabled=(DEVICE.type == 'cuda')):\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "                \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({\"Loss\": loss.item()})\n",
    "            \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        metric.reset()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                with torch.autocast(device_type=DEVICE.type, enabled=(DEVICE.type == 'cuda')):\n",
    "                    outputs = torch.sigmoid(model(images))\n",
    "                metric.update(outputs, labels.long())\n",
    "        \n",
    "        val_aucs = metric.compute()\n",
    "        mean_val_auc = torch.nanmean(val_aucs).item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val mAUROC: {mean_val_auc:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        log_dict = {\n",
    "            \"epoch\": epoch, \n",
    "            \"train_loss\": avg_train_loss, \n",
    "            \"val_mAUC\": mean_val_auc, \n",
    "            \"lr_finetune\": scheduler.get_last_lr()[0]\n",
    "        }\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            log_dict[f\"val_auc_{class_name}\"] = val_aucs[i].item()\n",
    "        wandb.log(log_dict)\n",
    "\n",
    "        if mean_val_auc > best_val_auc:\n",
    "            best_val_auc = mean_val_auc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), f\"./vit-finetuned-best.pt\")\n",
    "            print(f\"New best model saved with mAUROC: {best_val_auc:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= cfg[\"early_stopping_patience\"]:\n",
    "            print(f\"Early stopping triggered after {epochs_no_improve} epochs with no improvement.\")\n",
    "            break\n",
    "\n",
    "    print(\"Phase 2 finished.\")\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f851f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_train(config, PRETRAINED_BACKBONE_PATH)\n",
    "supervised_finetune(config, PRETRAINED_BACKBONE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
