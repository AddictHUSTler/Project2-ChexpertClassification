{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b81b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, classification_report, roc_curve, auc\n",
    "import timm\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff17f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningViT(nn.Module):\n",
    "    def __init__(self, lora_config, num_classes=14, drop_rate=0.1):\n",
    "        super(FineTuningViT, self).__init__()\n",
    "        backbone = timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=0, drop_rate=drop_rate)\n",
    "        self.backbone = get_peft_model(backbone, lora_config)\n",
    "        self.classifier = nn.Linear(self.backbone.embed_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.backbone(x))\n",
    "\n",
    "class CheXpertTestDataset(Dataset):\n",
    "    def __init__(self, data, root_dir, transforms):\n",
    "        self.labels = torch.tensor(data.values.astype(np.float32))\n",
    "        self.root_dir = root_dir\n",
    "        self.img_paths = [os.path.join(self.root_dir, img_path) for img_path in data.index]\n",
    "        self.transform = transforms\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = self.img_paths[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            return self.transform(image=np.array(image))['image'], self.labels[idx]\n",
    "        except (IOError, FileNotFoundError):\n",
    "            return torch.zeros((3, 224, 224)), torch.zeros(14)\n",
    "\n",
    "def get_predictions(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Getting Predictions\"):\n",
    "            outputs = torch.sigmoid(model(images.to(device)))\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    return np.vstack(all_preds), np.vstack(all_labels)\n",
    "\n",
    "def calculate_f1_thresholds(y_true, y_pred, classes):\n",
    "    \"\"\"\n",
    "    This function finds the single best threshold for each disease by\n",
    "    maximizing the standard F1-score (beta=1.0).\n",
    "    \"\"\"\n",
    "    optimal_thresholds = {}\n",
    "    print(\"\\nCalculating optimal thresholds by maximizing F1-score...\")\n",
    "    for i, class_name in enumerate(classes):\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true[:, i], y_pred[:, i])\n",
    "        \n",
    "        if len(thresholds) == 0:\n",
    "             optimal_thresholds[class_name] = 0.5\n",
    "             continue\n",
    "\n",
    "        f1_scores = (2 * precision * recall) / (precision + recall + 1e-8)\n",
    "        \n",
    "        if len(f1_scores) > 1:\n",
    "            best_idx = np.argmax(f1_scores[:-1])\n",
    "            optimal_threshold = thresholds[best_idx]\n",
    "        else: \n",
    "            optimal_threshold = 0.5\n",
    "        \n",
    "        optimal_thresholds[class_name] = optimal_threshold\n",
    "        print(f\"  - {class_name}: Optimal Threshold = {optimal_threshold:.4f}\")\n",
    "    return optimal_thresholds\n",
    "\n",
    "def plot_auroc_curves(y_true, y_pred, classes, model_name):\n",
    "    \"\"\"\n",
    "    Calculates and plots the AUROC curve for each class directly in the notebook.\n",
    "    Also calculates and returns the mean AUROC score.\n",
    "    \"\"\"\n",
    "    print(\"\\nGenerating AUROC curves...\")\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    auroc_scores = []\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        if len(np.unique(y_true[:, i])) < 2:#edge case where the number of distinct samples is only 1, which is unlikely to happen\n",
    "            print(f\"  - Skipping {class_name}: not enough distinct samples.\")\n",
    "            continue\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auroc_scores.append(roc_auc)\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'{class_name} (AUC = {roc_auc:.4f})')\n",
    "\n",
    "    mean_auroc = np.mean(auroc_scores)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Chance (AUC = 0.5000)')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title(f'AUROC for {model_name} (Mean AUC = {mean_auroc:.4f})', fontsize=16)\n",
    "    plt.legend(loc=\"lower right\", fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    print(f\"Displayed AUROC plot for {model_name}\")\n",
    "    return mean_auroc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2179454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the model evaluation pipeline. It now shows plots\n",
    "    inline and prints the mean AUROC score.\n",
    "    \"\"\"\n",
    "    all_model_names = [\n",
    "        'ViT-LoRA-U0',\n",
    "        'ViT-LoRA-U1',\n",
    "        'ViT-LoRA-contrastive-U0',\n",
    "        'ViT-LoRA-contrastive-U1'\n",
    "    ]\n",
    "    \n",
    "    classes = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n",
    "\n",
    "    for model_name in all_model_names:\n",
    "        print(f\"\\n{'='*20} Processing model: {model_name} {'='*20}\")\n",
    "        \n",
    "        CHECKPOINT_DIR, DEVICE = '', torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        LORA_CONFIG = LoraConfig(r=64, lora_alpha=256, target_modules=[\"qkv\", \"proj\"], lora_dropout=0.1, bias=\"none\")\n",
    "        \n",
    "        model_path = os.path.join(CHECKPOINT_DIR, f\"{model_name}.pth\")\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Error: Model file not found for {model_name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if 'contrastive' in model_name:\n",
    "            model = FineTuningViT(lora_config=LORA_CONFIG, num_classes=14)\n",
    "        else:\n",
    "            model = get_peft_model(timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=14), LORA_CONFIG)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "\n",
    "        u_version = model_name[-1]\n",
    "        test_csv_path = f'u{u_version}_test.csv'\n",
    "        if not os.path.exists(test_csv_path):\n",
    "            print(f\"Error: Validation CSV not found for {model_name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        test_df = pd.read_csv(test_csv_path, index_col=0)\n",
    "        test_df.index = test_df.index.str.replace('CheXpert-v1.0-small', 'chexpert')\n",
    "        transforms = A.Compose([A.Resize(224, 224), A.Normalize(mean=[0.506, 0.506, 0.506], std=[0.287, 0.287, 0.287]), ToTensorV2()])\n",
    "        test_dataset = CheXpertTestDataset(test_df, root_dir='', transforms=transforms)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "        y_pred, y_true = get_predictions(model, test_loader, DEVICE)\n",
    "        \n",
    "        # Plot AUROC curves and get the mean score.\n",
    "        mean_auroc = plot_auroc_curves(y_true, y_pred, classes, model_name)\n",
    "        print(f\"\\n  -> Mean AUROC Score on Test Set: {mean_auroc:.4f}\")\n",
    "\n",
    "        new_thresholds = calculate_f1_thresholds(y_true, y_pred, classes)\n",
    "        \n",
    "        y_pred_final = np.zeros_like(y_pred)\n",
    "        for i, class_name in enumerate(classes):\n",
    "            y_pred_final[:, i] = (y_pred[:, i] >= new_thresholds.get(class_name, 0.5)).astype(int)\n",
    "        \n",
    "        report = classification_report(y_true, y_pred_final, target_names=classes, output_dict=True, zero_division=0)\n",
    "        macro_f1 = report['macro avg']['f1-score']\n",
    "        print(f\"\\n  -> Resulting Macro Avg F1-Score on Test Set: {macro_f1:.4f}\")\n",
    "        output_filename = os.path.join(CHECKPOINT_DIR, f'{model_name}_best_tuned.csv')\n",
    "        pd.DataFrame(list(new_thresholds.items()), columns=['class', 'threshold']).to_csv(output_filename, index=False)\n",
    "        print(f\"Successfully saved final F1-tuned thresholds to '{output_filename}'\")\n",
    "\n",
    "    print(f\"\\n{'='*20} All threshold files have been generated and AUROC plots displayed. {'='*20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b51558",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
